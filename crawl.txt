import asyncio
import os
import random
import time

import aiofiles
import aiohttp
import async_timeout
from lxml import etree


async def getPage(url):
    headers = {
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
        'Accept-Encoding': 'gzip, deflate, br',
        'Accept-Language': 'zh-CN,zh;q=0.9',
        'Connection': 'keep-alive',
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36',
        'Host': '',
        'Cookie': 'auto_page=1; __atuvc=1%7C13; _pk_ses.3.76e6=1; _pk_id.3.76e6=75ead033575a02da.1553523092.3.1553865397.1553865160.',
        'Referer': '',
        'Upgrade-Insecure-Requests': '1'
    }
    conn = aiohttp.TCPConnector(verify_ssl=False)
    async with aiohttp.ClientSession(connector=conn) as session:
        async with session.get(url, headers=headers) as contentresp:
            status = contentresp.status
            contentrespdata = await contentresp.text()
            while status != 200:
                headers = {
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
                    'Accept-Encoding': 'gzip, deflate, br',
                    'Accept-Language': 'zh-CN,zh;q=0.9',
                    'Connection': 'keep-alive',
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36',
                    'Host': '',
                    'Cookie': 'auto_page=1; __atuvc=1%7C13; _pk_ses.3.76e6=1; _pk_id.3.76e6=75ead033575a02da.1553523092.3.1553865397.1553865160.',
                    'Referer': '',
                    'Upgrade-Insecure-Requests': '1'
                }
                conn = aiohttp.TCPConnector(verify_ssl=False)
                async with aiohttp.ClientSession(connector=conn) as session:
                    async with session.get(url, headers=headers) as contentresp:
                        status = contentresp.status
                        contentrespdata = await contentresp.text()

            response = contentrespdata.decode()
            htl = bytes(bytearray(response, encoding='utf-8'))
            selector = etree.HTML(htl)
            try:
                link = selector.xpath('//*[@id="highres"]/@href')[0]
            except Exception:
                pass
            else:
                headers = {
                    ':authority': '',
                    ':method': 'GET',
                    ':path': str(str(link)[23:]),
                    ':scheme': 'https',
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
                    'Accept-Encoding': 'gzip, deflate, br',
                    'Accept-Language': 'zh-CN,zh;q=0.9',
                    'Connection': 'keep-alive',
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36',
                    'Referer': str(url),
                    'Upgrade-Insecure-Requests': '1'
                }
                local_filename = str(link).split('/')[-1]
                async with aiohttp.ClientSession(headers=headers) as session_detail:
                    async with session_detail.get("https:" + str(link)) as res:
                        with open("./" + local_filename.split('?')[0], 'wb') as f:
                            while 1:
                                chunk = await res.content.read(8192)
                                if not chunk:
                                    break
                                # lp = asyncio.get_event_loop()
                                # lp.run_in_executor(None, save_file, fd, chunk)
                                f.write(chunk)
                            f.close()
                            print("https:" + str(link))


if __name__ == '__main__':
    batch = 200
    for batch_v in range(0, 800000, batch):
        urllist = ['' + str(page) for page in range(batch_v, batch_v + batch)]
        loop = asyncio.get_event_loop()
        tasks = [getPage(url) for url in urllist]
        loop.run_until_complete(asyncio.wait(tasks))
